Symbolic Matrix Diagonalization in CAS
Challenges of Symbolic Diagonalization

Symbolically diagonalizing a matrix is a non-trivial task due to fundamental algebraic limitations. The core issue is that finding eigenvalues requires solving the characteristic polynomial $\det(A - \lambda I)=0$. For a general $n\times n$ matrix, this is an $n$th-degree polynomial. By the Abel–Ruffini theorem, there is no general closed-form solution in radicals for polynomials of degree $\ge 5$. In practical terms, this means that for matrices of size 5 or larger, one cannot expect a simple expression for eigenvalues (and thus the diagonal form) in terms of the matrix’s entries using elementary functions. Any algorithm attempting full symbolic diagonalization must confront this hurdle.

Early comments in the literature note that symbolic diagonalization “includes as a special case the symbolic expression of the zeros of a polynomial, which is problematic for degree $\ge 5$”. In other words, beyond $4\times4$ matrices we hit the unsolvability of the general eigenvalue problem in closed form. However, this does not mean all hope is lost – it just means a general formula is impossible. We can still represent eigenvalues symbolically in implicit ways, or diagonalize special classes of matrices where structure simplifies the problem.
Approaches in Computer Algebra Systems (CAS)

Modern CAS (Computer Algebra Systems) like Mathematica, Maple, Sympy, etc., do provide functions to diagonalize matrices symbolically when possible. They circumvent the Abel–Ruffini barrier by using implicit algebraic representations for eigenvalues rather than explicit radical formulas. For example, Mathematica and Maple will represent unsolvable eigenvalues using a special Root object that encapsulates “the $k$-th root of a given polynomial”. Instead of a long formula, the CAS returns something like λ = Root(f(λ), k), which indicates an eigenvalue is the $k$-th root of the polynomial $f$. This is effectively an exact symbolic representation (in terms of a minimal polynomial) without a closed-form numeric expression. As one MathOverflow discussion explains, “Mathematica and Maple sidestep the problem of 'symbolic expression' of roots by using a data structure containing the minimal polynomial … and an index”. In other words, the CAS will yield eigenvalues in terms of their minimal polynomials rather than elementary radicals when necessary.

Using such implicit representations, CAS can symbolically diagonalize a matrix by producing a similarity transformation $A = PDP^{-1}$ where $D$ is diagonal (or block-diagonal) with those Root objects on the diagonal. The matrix $P$ (eigenvector matrix) will also contain those root terms. This is exactly what Mathematica’s Diagonalize or JordanDecomposition functions do. They do not literally “solve” a 5th-degree polynomial; instead, they return results in terms of the polynomial’s roots. For instance, Mathematica’s JordanDecomposition[] “gives exact symbolic answers in cases when the eigenvalues are expressed by simple expressions of the symbols, even if the characteristic polynomial is of higher order”. When the eigenvalues cannot be expressed in a simpler closed form, the result will involve those Root objects. Maple’s Eigenvalues function behaves similarly, returning solutions involving its RootOf construct for higher-degree cases.

Key point: CAS can perform symbolic diagonalization, but the result may include abstract algebraic numbers (roots of polynomials) rather than elementary expressions. This approach is not hindered by Abel–Ruffini because it doesn’t insist on a radical expression – it leaves the eigenvalues in an implicit form that is exact. In summary, while we cannot always diagonalize with a “nice” formula, we can represent the diagonalization symbolically within an algebraic extension field.
Literature and Algorithms for Symbolic Diagonalization

There is a substantial body of research on algorithms to compute matrix canonical forms symbolically. These algorithms aim to diagonalize or partially diagonalize matrices without numerical approximation and are documented in the scientific literature (often in the context of computer algebra and symbolic computation). Some highlights:

    Rational Canonical Form (Frobenius Normal Form): A common strategy is to compute the rational canonical form (also known as Frobenius form) of the matrix as an intermediate step. This form is a block-diagonal matrix where each diagonal block is a companion matrix of an irreducible factor of the characteristic polynomial. Essentially, it groups the eigenvalues according to their minimal polynomials. If the characteristic polynomial factors into, say, $(p_1(x))^{m_1}(p_2(x))^{m_2}\cdots$, the rational canonical form is block-diagonal with companion matrices for $p_1, p_2, \ldots$. This is a symbolic block diagonalization: each block corresponds to an invariant subspace associated with one irreducible factor. Computing this form does not require solving the polynomial factors; it only requires polynomial arithmetic (factoring and GCDs). In a fully diagonalizable case where all factors are linear, this yields a diagonal matrix of eigenvalues. If some factors are higher-degree (e.g. an irreducible 5th-degree polynomial), that block remains as a 5×5 companion matrix – effectively acknowledging “we can’t simplify this part further” (thus not violating Abel–Ruffini). The matrix is thus block-diagonalized as far as possible. Several papers in the 1980s and 1990s describe algorithms for computing this form efficiently.

    Early algorithmic work: Kaltofen, Krishnamurthy, and Saunders (1986) gave “fast parallel algorithms for similarity of matrices” – essentially an algorithm to compute a canonical form in polynomial time
    csd.uwo.ca
    . P. Ozello’s 1987 PhD thesis developed exact computation of Jordan and Frobenius forms
    csd.uwo.ca
    . Work by D. Gilat (1992) and others in the early 90s continued to refine the approach for computing Jordan forms in a CAS (e.g. using the Axiom system). By the mid-90s, Giesbrecht (1995) published a nearly optimal algorithm for matrix canonical forms, showing that one can compute the Frobenius form and even the Jordan form (over algebraic extensions) in randomized polynomial time
    csd.uwo.ca
    . These works introduced techniques like using the Frobenius form as a stepping stone to the Jordan form and gave complexity bounds for these operations. In essence, they proved that symbolic diagonalization (in the sense of finding a form of $A = VDV^{-1}$ over an extension field) is algorithmically feasible in polynomial time, even though it may involve adjoint algebraic numbers rather than radicals.

    Jordan Normal Form vs. Rational Form: Full Jordan normal form requires not just eigenvalues but also their geometric multiplicities (eigenvectors and possibly Jordan chains for defective matrices). Symbolic algorithms handle this by working with minimal polynomials and nullspace computations. One practical approach (used in CAS) is to compute the minimal polynomial and the companion matrix form (Frobenius form), and then refine that to Jordan form if needed by splitting off blocks for each eigenvalue. For diagonalization, if the matrix is diagonalizable, each companion block is 1×1 (linear factor) and we’re done. If not, one ends up with larger Jordan blocks. The key takeaway is that these algorithms yield a similarity transformation to block-diagonal form where each block corresponds to an irreducible polynomial factor. This aligns exactly with the idea of avoiding Abel–Ruffini by not decomposing irreducible factors further. It provides a symbolic block-diagonalization that is optimal given the algebraic constraints.

    Complexity considerations: Recent surveys (e.g. N. Srivastava 2023) note that while exact eigen-decomposition with radicals is impossible in general, one can compute an exact representation via minimal polynomials. The algorithms mentioned (Kaltofen et al., Giesbrecht, Storjohann, etc.) have polynomial complexity in the bit-length of entries for computing these forms. This is significant because it means symbolic diagonalization isn’t just a theoretical curiosity – it can be done efficiently for reasonably sized matrices, especially if the matrix has some structure or if we settle for the rational canonical form. In practice, CAS will attempt these methods: factor the characteristic/minimal polynomial as much as possible, and report the result in terms of factors or Root objects if needed.

In summary, the literature provides algorithmic foundations for symbolic diagonalization: one can find references such as Kaltofen et al. 1986
csd.uwo.ca
, Gilat 1992, Giesbrecht 1995
csd.uwo.ca
, Storjohann 2003, and others that describe how to compute diagonal or Jordan forms in a computer algebra setting. These methods are implemented (at least in part) in systems like Mathematica and Maple – for example, Maple’s JordanForm uses polynomial factorization and linear algebra to get a result, often involving “uncomputed” eigenvalues represented abstractly. The key is acknowledging when an eigenvalue can’t be simplified further: the CAS will leave it as an algebraic number (an eigenvalue represented by its minimal polynomial) rather than a radical expression.
Pattern Matching and Special Cases for Diagonalization

One way to beat Abel–Ruffini in practice is to leverage special structure in matrices. Many matrices that arise in applications are not “generic” random matrices; they have patterns (symmetries, zero structure, parameter relationships) that make their eigenproblems more tractable. CAS can use pattern matching heuristics to detect these cases and apply formulae or transformations specific to the pattern. Here are a few important examples:

    Low-Dimensional Matrices: For very small matrices (2×2, 3×3, 4×4), there are known closed-form solutions for eigenvalues. A CAS will use the quadratic formula for 2×2, the analytical cubic solution for 3×3, and Ferrari’s quartic solution for 4×4. These yield explicit expressions for eigenvalues (and eigenvectors) in terms of the entries. For instance, a $2\times2$ symmetric matrix $\begin{pmatrix}a & b\ b & c\end{pmatrix}$ has eigenvalues $ \frac{a+c}{2} \pm \frac{\sqrt{(a-c)^2+4b^2}}{2}$ – a CAS pattern-matches to this form immediately. Similarly, for 3×3, formulas involving cube roots and arccosine (for three real eigenvalues) are applied. This covers many textbook cases and is implemented in CAS. Once eigenvalues are found, forming the diagonalization is straightforward by solving $(A - \lambda_i I)x=0$ for each. (CAS will do this symbolically, which can get messy but is doable for small n.)

    Circulant Matrices: A circulant matrix (one that is constant along each circular diagonal) is a classic example of a matrix that is analytically diagonalizable by a discrete Fourier transform. If $C$ is circulant, there is a known closed form for all its eigenvalues in terms of roots of unity. Specifically, for an $n\times n$ circulant matrix $C = \mathrm{circ}(c_0, c_1, \dots, c_{n-1})$, the eigenpairs are given by Fourier modes: the $j$-th eigenvector has components $(1, \omega^j, \omega^{2j}, \dots, \omega^{(n-1)j})$ (with $\omega = e^{2\pi i/n}$), and the corresponding eigenvalue is $\lambda_j = c_0 + c_1 \omega^{-j} + c_2 \omega^{-2j} + \cdots + c_{n-1}\omega^{-(n-1)j}$. A CAS can recognize a circulant pattern and instantly write down this solution. Notice that this formula includes $\omega^k$, which is an $n$th root of unity – an algebraic number. This is entirely within the scope of symbolic representation (roots of unity are solvable algebraic numbers), so it’s a closed form. Many special matrices (like DFT matrices, companion matrices of cyclotomic polynomials, etc.) fall into this category of being diagonalizable by known Fourier-like transforms.

    Toeplitz and Other Structured Matrices: Another well-known case: symmetric Toeplitz tridiagonal matrices (a common form in numerical linear algebra) have eigenvalues that can be written in closed form. If $T_n$ is an $n\times n$ tridiagonal Toeplitz matrix with constant diagonal $a$ and constant off-diagonals $b$ (subdiagonal) and $b$ (superdiagonal), then the eigenvalues are given by $\displaystyle \lambda_k = a + 2b\cos!\frac{k\pi}{n+1}$ for $k=1,2,\dots,n$. (More generally, if the off-diagonals are $b$ and $c$, $\lambda_k = a \pm 2\sqrt{b,c}\cos(\frac{k\pi}{n+1})` as appropriate.) This formula comes from the theory of special orthogonal polynomials and the fact that such matrices are essentially discretized second-derivative operators with Dirichlet boundary conditions. A CAS that detects this pattern can output the eigenvalues in this neat trigonometric form. Indeed, the presence of a cosine (or any simple function) of $k\pi/(n+1)$ is far simpler than a generic RootOf an $n$th degree polynomial. Many other structured matrices have known eigen-spectra: for example, Hadamard matrices, companion matrices of certain polynomials, Vandermonde matrices, etc., have specialized results. Pattern-matching these can give symbolic diagonalizations that are otherwise hard if approached blindly. (A research article notes, however, that “in general, tridiagonal matrices with closed form eigenvalues are rare – most known examples are just variations of the symmetric Toeplitz case”, underscoring that we rely on recognizing a special pattern.)

    Exploiting Symmetry (Group Theory): Perhaps the most powerful pattern-matching tool is exploiting symmetry. If a matrix (especially a Hermitian matrix) has a symmetry group, you can block-diagonalize it by switching to a basis of irreducible representations of that group. This is a classic technique in physics and chemistry for Hamiltonians and in engineering for structural matrices. The idea is that if a matrix $A$ commutes with a set of permutation or symmetry operations, then $A$ and those symmetry operators can be simultaneously block-diagonalized. In practice, one uses group representation theory: construct projection operators (idempotents) from the group’s character table to project the space onto invariant subspaces. By applying these projections, one obtains a transformation matrix $T$ (orthogonal/unitary) that converts $A$ into a direct sum of smaller matrices $A_{(1)} \oplus A_{(2)} \oplus \cdots$. Each block $A_{(i)}$ corresponds to one irreducible representation of the symmetry group. “The transformation matrix $T$, which has orthogonal properties, is used to convert the system matrix into block-diagonal form. Each block corresponds to an independent subspace.” This greatly reduces the problem size – instead of one $n\times n$ eigenproblem, you have several smaller ones (whose total size is $n$). Crucially, the characteristic polynomials of these smaller blocks divide the original polynomial, often making them easier to solve (and sometimes each block corresponds to a repeated eigenvalue that is obvious by symmetry). This approach is not hindered by Abel–Ruffini because you are leveraging additional information to simplify the polynomial. It’s a pattern-matching at the level of matrix structure: the CAS (or the user) recognizes the matrix has a symmetry (e.g. a block circulant structure, or invariance under certain permutations) and uses a known change-of-basis to block-diagonalize it. There is ongoing research on automating this process. For example, a recent 2023 paper presents a “consistent group-theoretic strategy for block-diagonalization of structural matrices”, explicitly constructing the symmetry-adapted basis for engineering structures. Such methods find direct application in simplifying large Hermitian matrices (common in vibration analysis, quantum chemistry, etc.) by splitting them into smaller blocks that can be diagonalized individually.

    Commuting Matrices (Simultaneous Diagonalization): A related scenario is if you have not just one matrix, but a whole family of matrices that commute. In that case, there is a basis that simultaneously diagonalizes all of them (at least block-diagonalizes if they don’t have a complete set of common eigenvectors). If a matrix $A$ is part of a commuting set, one can often diagonalize it by diagonalizing a simpler matrix in the set and using the same basis for $A$. For example, $A$ might be a function of another matrix $B$ (say $A = p(B)$ for some polynomial $p$). Then $A$ and $B$ share eigenvectors; diagonalizing $B$ (which might be easier) immediately diagonalizes $A$. In symbolic terms, if $B$ has a known spectral decomposition $B = UDU^{-1}$, then $A = p(B) = U p(D) U^{-1}$ is diagonal in the same basis. CAS could use this if it detects such a relationship (this is another form of pattern matching). A concrete instance: any polynomial matrix function $p(B)$ is diagonalized by diagonalizing $B$. So if $A$ is given as $3I + 2B^2$, one would diagonalize $B$ (if possible) and then just square the eigenvalues to get $A$’s eigenvalues.

In summary, pattern matching in this context means identifying when a matrix falls into a class with known diagonalization methods. This could be via algebraic structure (circulant, Toeplitz, rank-one update, etc.), via symmetry (block structure under permutation similarity, group invariants), or via functional relationships (commuting matrices, polynomial in a simpler matrix). By exploiting these, a CAS can block-diagonalize even higher-dimensional matrices effectively, avoiding the generic unsolvable cases. This is how one can get around Abel–Ruffini in practice: the problem is only unsolvable in general, but many specific instances are solvable due to additional structure. A well-designed symbolic algorithm will check for these solvable instances. (Indeed, Mathematica’s algorithms are known to have many built-in heuristics for recognizing special matrices and applying tailored methods.)
Eigenvalue Decomposition vs. Singular Value Decomposition (SVD)

The question specifically asks about “all kinds of diagonalization, in particular eigenvalue factorization and SVD,” so it’s worth distinguishing these:

    Eigenvalue (Spectral) Decomposition: This is the usual $A = VDV^{-1}$ (or $A=UDU^\dagger$ for unitary diagonalization of normal matrices) where $D$ contains eigenvalues. The discussion above has essentially been about this. For Hermitian matrices (or real symmetric matrices), $V$ can be chosen unitary (orthogonal) and $D$ will be real diagonal. All Hermitian matrices are diagonalizable by a unitary transformation (spectral theorem), so there is no issue of non-diagonalizability or Jordan forms in that case. The primary difficulty is finding the eigenvalues symbolically. In Hermitian cases, one advantage is that eigenvalues are real and often one can order or approximate them by analytical methods (e.g. perturbation theory or series expansions if the matrix depends on a parameter). However, from a purely algebraic standpoint, a generic Hermitian matrix of size $\ge5$ is just as hard to solve explicitly as a non-symmetric one (its characteristic polynomial is still degree $n$). The CAS methods do not fundamentally change for Hermitian matrices, aside from possibly using the fact that the minimal polynomial splits into linear factors over $\mathbb{R}$ if we allow real algebraic extensions (the eigenvalues are real algebraic numbers). The applications for Hermitian matrices are abundant – quantum mechanics is a prime example, where matrices (Hamiltonians) are Hermitian and one is interested in symbolic or semi-symbolic eigenanalysis. There, these techniques of symmetry exploitation and partial diagonalization are extremely useful. For instance, a common task is to block-diagonalize a Hamiltonian by exploiting conservation laws or symmetries (essentially the group-theoretic approach noted above) before resorting to numeric eigen-solvers on each block.

    Singular Value Decomposition (SVD): The SVD $A = U \Sigma V^\dagger$ is another type of diagonalization – not of $A$ itself, but of $A^A$ and $AA^$. The singular values (the entries of $\Sigma$) are the square roots of the eigenvalues of $A^*A$. So computing an SVD symbolically is closely related to computing an eigen-decomposition of a positive semi-definite Hermitian matrix $A^*A$. In principle, one could apply all the same techniques: find the eigenvalues of $A^*A$ symbolically (they are $\sigma_i^2$), then take square roots to get singular values $\sigma_i$, and find the corresponding singular vectors by solving $(A^A - \sigma^2 I)u=0$ and $(AA^ - \sigma^2 I)v=0$. However, if $n$ is large, $\det(A^*A - \lambda I)=0$ is again a high-degree polynomial (degree $n$ in $\lambda$). Thus, beyond small cases, symbolic SVD faces the same Abel–Ruffini barrier. Indeed, even if $A$ is $5\times5$, the equation for singular values $\det(A^*A - \lambda I)=0$ is generally degree 5, not solvable in radicals. The CAS will then represent singular values using Root objects (just as it would eigenvalues). In fact, many CAS do not have a dedicated “symbolic SVD” function – rather, one would just compute the eigenvalues of $A^*A$ and proceed. For example, if you ask Sympy or Mathematica for an SVD of a symbolic matrix, they might internally do that or simply return a numeric SVD if you give numeric values.

    That said, there are special cases where SVD is easier than full eigen-decomposition: for instance, if $A$ is a real matrix with certain structure (like an off-diagonal symmetric form), sometimes the singular values have symmetric expressions. But generally, SVD doesn’t avoid any complexity, it’s essentially an eigen-problem in disguise. One difference is that singular values are always non-negative and one can sometimes get away with solving a quadratic equation for $\sigma$ if the eigenvalue problem factorizes nicely (since $\sigma$ appears squared). For example, for a $2\times2$ matrix, the eigenvalues of $A^T A$ satisfy a quadratic equation, and one can get a simple closed form for $\sigma_{1,2}$. For larger structured matrices (like bidiagonal or block diagonal matrices), the singular values might be determined by smaller characteristic polynomials.

    In summary, if one wants a symbolic SVD, one typically: (1) symbolically computes $A^*A$ (which is Hermitian), (2) finds its eigenvalues $\lambda_1,\dots,\lambda_n$ symbolically (as discussed, possibly in Root form), and then (3) takes $\sigma_i = \sqrt{\lambda_i}$ (choosing the positive root). The matrices $U$ and $V$ of singular vectors can be found by solving linear equations $Av_i = \sigma_i u_i$ etc., which is analogous to finding eigenvectors. This is doable if $\sigma_i$ are known symbolically. In practice, CAS will often avoid doing a full SVD symbolically unless specifically asked – it’s more common to do eigen-decomposition. But the theory and algorithms are analogous. For special applications like symbolic analysis of least-squares problems or control theory, one might derive formulas for singular values of certain $3\times3$ or $4\times4$ symbolic matrices (there is some literature on closed-form SVD for $2\times2$ and $3\times3$ cases, focusing on numeric stability). Again, pattern recognition can help: e.g. if $A$ is diagonal or block-diagonal, the singular values are just absolute values of the diagonal entries (trivial case), or if $A$ has rank 1 (then it has only one non-zero singular value which is easy to find), etc. CAS can identify such cases and give results quickly.

Applications and Special Considerations (especially for Hermitian matrices)

Symbolic diagonalization is not just a theoretical exercise – it has real applications where an analytic result is valuable:

    Differential Equations and Systems: If you can diagonalize a system matrix symbolically, you can explicitly solve systems of linear ODEs. For example, in control theory or mechanical systems, having an analytic eigen-decomposition of the system matrix allows writing down solutions for all time (matrix exponentials) in closed form. This is feasible for small systems or those with structure (like normal modes of a symmetric system).

    Quantum Mechanics (Hermitian matrices): Finding eigenvalues and eigenvectors of Hamiltonians symbolically can help understand the parametric dependence of energy levels, avoid numerical instabilities, and provide exact expressions for critical values. Hermitian matrices often depend on parameters (e.g. coupling constants) and one might perform a symbolic perturbation analysis. In such cases, one might not fully diagonalize at once, but rather use series expansions. Nonetheless, block-diagonalizing the Hamiltonian by symmetry (as discussed) is a crucial first step. Applications like computing symbolic spectra of small molecules or spin systems use these techniques. There’s even specialized research on degenerate perturbation theory symbolically – essentially ensuring a reliable diagonalization when eigenvalues are equal or very close. The cited “Reliable Degenerate Matrix Diagonalization” (Park and Ziegler 2018) deals with the difficulties of symbolic computation when eigenvalues have multiplicities or near-multiplicities, ensuring the algorithm correctly identifies independent eigenvectors in the degenerate case (which can be tricky numerically).

    Matrix Functions: Diagonalizing a matrix symbolically also allows one to compute functions of the matrix (like $e^A$, $\sin A$, $A^p$, etc.) in closed form by simply applying the function to the eigenvalues. This is another motivation in engineering and math (Higham’s book is a reference on functions of matrices). If one can get $A = UDU^{-1}$, then $f(A) = U f(D) U^{-1}$ with $f(D)$ trivial to compute (just apply $f$ to each diagonal entry). Thus, symbolic diagonalization enables symbolic computation of matrix exponentials, powers, etc., when possible. For example, the matrix exponential of a $3\times3$ matrix with a full set of distinct symbolic eigenvalues can be written in closed form using those eigenvalues.

    CAS Development: The question mentions “we want to develop it on our own”, which suggests implementing these algorithms without relying on external CAS. The literature cited (Kaltofen et al., Giesbrecht, etc.) provides the theoretical foundation for doing so. Practically, one would implement polynomial factorization (over rationals or symbolically), minimal polynomial computation (e.g. via the Frobenius form computation), and then construction of the similarity transform. Many of the references are in conference proceedings like ISSAC (International Symposium on Symbolic and Algebraic Computation), indicating this is an active area of research in computer algebra. For instance, the algorithm by Giesbrecht (1995)
    csd.uwo.ca
    or the improvements by Storjohann (1998, 2000) could be guides to implementing an efficient diagonalizer. Additionally, modern CAS use heuristics – e.g. try a numeric algorithm with high precision to guess an exact result (a technique called dynamic evaluation or integer reconstruction). This means they might compute eigenvalues numerically to high precision and attempt to recognize them as algebraic numbers (this works if the matrix entries are rational or algebraic). Such hybrid methods can be very powerful: they give a way to guess the minimal polynomial of an eigenvalue and then confirm it symbolically.

In the specific context of Hermitian matrices, one often has additional structure to exploit: Hermitian matrices commonly arise from physical symmetry (like a lattice with periodic boundary – leading to circulant matrices, solved by Fourier analysis; or a molecule with point-group symmetries – solved by group representation theory as noted). Also, all eigenvalues are real, so one might use Sturm sequences or other real-root isolation techniques to at least locate them symbolically. While these don’t give a closed form, they give qualitative insight (e.g. monotonicity in a parameter, etc.). For applications like optimization or control (where one might need symbolic gradients of eigenvalues), having an implicit symbolic form is useful: one can differentiate through the eigen-decomposition if it’s given in terms of symbolic roots.

To wrap up, symbolic diagonalization (including eigen-decomposition and SVD) is a blend of deep algebraic theory and clever pattern-based heuristics. The Abel–Ruffini theorem tells us a blanket formula is impossible, but by focusing on structured cases and algebraic representations, one can diagonalize a surprisingly broad array of matrices symbolically:

    CAS handle general cases by representing eigenvalues with minimal polynomials (avoiding explicit unsolvable expressions).

    There are published algorithms that run in polynomial time to compute canonical forms, using factorizations and companion matrices. These form the backbone of any custom implementation.

    Special structures (circulant, Toeplitz, symmetric/Hermitian with patterns) allow closed-form solutions for large $n$ that are tractable. Recognizing these patterns is key – effectively a form of symbolic pattern matching.

    Using symmetry to block-diagonalize matrices with unitary transformations is a powerful way to reduce complexity, and literature in engineering and physics demonstrates how this yields smaller sub-problems that are solvable. This is not hindered by Abel–Ruffini because it never attempts to solve an irreducible polynomial of high degree; it reduces the degree of the polynomials by exploiting symmetry. For example, a full $n$th degree polynomial might factor into one quadratic and one $(n-2)$th degree polynomial if a symmetry splits a matrix into a $2\times2$ block and an $(n-2)\times(n-2)$ block.

In conclusion, there is a rich theory and literature on symbolic matrix diagonalization. By combining these theoretical algorithms with pattern-matching heuristics, one can develop a CAS module that symbolically diagonalizes matrices (for eigen decomposition and SVD) in many useful cases. This includes fully diagonalizing matrices that are diagonalizable with solvable eigenvalues, or block-diagonalizing the rest into irreducible factors or symmetric subspaces when closed-form eigenvalues are unavailable. Such a tool would indeed “not be hindered by Abel–Ruffini” in the sense that it will find every simplification allowed by algebraic structure, and only leave the truly unsolvable parts in an implicit form. The references and techniques above should provide a strong starting point for developing this capability.

Sources: Contemporary research survey by N. Srivastava (2023); MathOverflow discussions on CAS eigenmethods; classic algorithms by Kaltofen et al. 1986
csd.uwo.ca
and Giesbrecht 1995
csd.uwo.ca
; examples of closed-form eigenvalues for structured matrices; and a group-theoretic block-diagonalization approach for symmetric matrices, among others. These illustrate both the limitations and the clever strategies to perform symbolic diagonalization in practice.
